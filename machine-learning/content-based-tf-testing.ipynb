{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'combined-dataset/final_reviews_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode the 'types' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "\n",
    "# Tokenize the 'review' column\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "\n",
    "# Pad the sequences\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create the feature set\n",
    "X = {\n",
    "    'review': padded_sequences,\n",
    "    'types': data['types_encoded'].values,\n",
    "}\n",
    "\n",
    "# Normalize the sentiment scores\n",
    "y = data['sentiment'].values\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:16:51.098925800Z",
     "start_time": "2024-06-12T15:16:47.790981100Z"
    }
   },
   "id": "4fce0614da3d08b4",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m547s\u001B[0m 665ms/step - loss: 0.8965 - mae: 0.5329 - val_loss: 0.0898 - val_mae: 0.2247\n",
      "Epoch 2/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m613s\u001B[0m 748ms/step - loss: 0.0527 - mae: 0.1662 - val_loss: 0.0548 - val_mae: 0.1634\n",
      "Epoch 3/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m646s\u001B[0m 789ms/step - loss: 0.0295 - mae: 0.1219 - val_loss: 0.0413 - val_mae: 0.1388\n",
      "Epoch 4/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m591s\u001B[0m 721ms/step - loss: 0.0163 - mae: 0.0901 - val_loss: 0.0374 - val_mae: 0.1339\n",
      "Epoch 5/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m501s\u001B[0m 611ms/step - loss: 0.0120 - mae: 0.0781 - val_loss: 0.0327 - val_mae: 0.1201\n",
      "Epoch 6/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m505s\u001B[0m 617ms/step - loss: 0.0095 - mae: 0.0683 - val_loss: 0.0301 - val_mae: 0.1160\n",
      "Epoch 7/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m506s\u001B[0m 617ms/step - loss: 0.0082 - mae: 0.0647 - val_loss: 0.0278 - val_mae: 0.1123\n",
      "Epoch 8/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m503s\u001B[0m 615ms/step - loss: 0.0064 - mae: 0.0566 - val_loss: 0.0264 - val_mae: 0.1076\n",
      "Epoch 9/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m480s\u001B[0m 586ms/step - loss: 0.0059 - mae: 0.0538 - val_loss: 0.0266 - val_mae: 0.1087\n",
      "Epoch 10/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m485s\u001B[0m 592ms/step - loss: 0.0047 - mae: 0.0500 - val_loss: 0.0245 - val_mae: 0.1056\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.history.History at 0x20ac6134290>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input layers\n",
    "review_input = Input(shape=(max_sequence_length,), name='review')\n",
    "types_input = Input(shape=(1,), name='types')\n",
    "\n",
    "# Define embedding and LSTM layers for review input\n",
    "review_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(review_input)\n",
    "review_lstm = LSTM(128)(review_embedding)\n",
    "\n",
    "# Define embedding layer for types input\n",
    "types_embedding = Embedding(input_dim=data['types_encoded'].nunique(), output_dim=10)(types_input)\n",
    "types_flat = tf.keras.layers.Flatten()(types_embedding)\n",
    "\n",
    "# Concatenate the review and types embeddings\n",
    "concatenated = Concatenate()([review_lstm, types_flat])\n",
    "\n",
    "# Add dense layers for final prediction\n",
    "dense_1 = Dense(128, activation='relu')(concatenated)\n",
    "dense_2 = Dense(64, activation='relu')(dense_1)\n",
    "output = Dense(1, activation='linear')(dense_2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[review_input, types_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T16:46:29.176501900Z",
     "start_time": "2024-06-12T15:16:51.096924800Z"
    }
   },
   "id": "16764ec0f1acbb1f",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1024/1024\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m195s\u001B[0m 190ms/step\n",
      "                                id  \\\n",
      "0      ChIJYcGr7GSb0S0RckePBrCWikw   \n",
      "26459  ChIJs5sHkDub0S0Rs6_ArAb6GYw   \n",
      "14966  ChIJoxOsRvY90i0RfdRmmjHBxDs   \n",
      "2445   ChIJVWVU7lI60i0ROjIjerOky-4   \n",
      "27601  ChIJ8QkA6MQi0i0RJwJqRqijNpc   \n",
      "1304   ChIJQTrID1IF0i0REGYdKOXKObk   \n",
      "3288   ChIJw5Yz3ZU50i0R2RDRqJsUvAs   \n",
      "26714  ChIJ5ZNHoDeJ0S0RhyP0FmUFXBQ   \n",
      "7369   ChIJ-UX-z8wg0i0R2t1WpZDLVDA   \n",
      "14808  ChIJ0__EiSM90i0Rh-u4ELIOCcQ   \n",
      "\n",
      "                                                types review_number  \\\n",
      "0                                      hotel, lodging      review 1   \n",
      "26459                                restaurant, food      review 5   \n",
      "14966            italian_restaurant, restaurant, food      review 3   \n",
      "2445          indonesian_restaurant, restaurant, food      review 1   \n",
      "27601  indonesian_restaurant, restaurant, food, store      review 5   \n",
      "1304       restaurant, coffee_shop, cafe, food, store      review 1   \n",
      "3288                                   hotel, lodging      review 1   \n",
      "26714          fast_food_restaurant, restaurant, food      review 5   \n",
      "7369          indonesian_restaurant, restaurant, food      review 2   \n",
      "14808               thai_restaurant, restaurant, food      review 3   \n",
      "\n",
      "                                                  review     user_id  \\\n",
      "0      It has quite small room, and the hallway is qu...  user_18425   \n",
      "26459  All the appetizers were delicious as well as t...   user_1070   \n",
      "14966  Best Italian restorant in Ubud period. Great l...   user_8888   \n",
      "2445   Regardless of the place called themself as Pad...  user_43900   \n",
      "27601  The fish is super good!! And it's affordable.\\...  user_19046   \n",
      "1304   Beautiful view, amazing food and super friendl...  user_36525   \n",
      "3288   Lovely hotel! So beautiful and aesthetic. We h...   user_5863   \n",
      "26714  It's a typical fastfood Ack friend chicken pla...  user_43551   \n",
      "7369   First tinder having babi guling , was deliciou...  user_27256   \n",
      "14808  We tried eating Thai food at this restaurant y...  user_26829   \n",
      "\n",
      "       sentiment  types_encoded  \n",
      "0           3.02            381  \n",
      "26459       3.83            719  \n",
      "14966       4.35            497  \n",
      "2445        3.00            469  \n",
      "27601       3.78            470  \n",
      "1304        3.98            709  \n",
      "3288        3.91            381  \n",
      "26714       2.72            274  \n",
      "7369        3.91            469  \n",
      "14808       3.86            858  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X['review'][place_idx]\n",
    "    place_types = X['types'][place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X['review'], X['types']])\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X['review'], X['types'].reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places\n",
    "\n",
    "# Example usage\n",
    "place_id = 'ChIJYcGr7GSb0S0RckePBrCWikw'  # Replace with an actual place ID from your dataset\n",
    "recommendations = get_recommendations(place_id, data, model, top_n=10)\n",
    "print(recommendations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T16:49:53.742500700Z",
     "start_time": "2024-06-12T16:46:35.759117400Z"
    }
   },
   "id": "ade66f54ef88bcd5",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('39_test_model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T17:03:43.131011700Z",
     "start_time": "2024-06-12T17:03:42.651602Z"
    }
   },
   "id": "6c28122d45aa642c",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e7096aa8d41a9b92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
