{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# from keras_tuner import RandomSearch, HyperModel\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'combined-dataset/final_reviews_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode the 'types' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "\n",
    "# Tokenize the 'review' column\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "\n",
    "# Pad the sequences\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create the feature set\n",
    "X = {\n",
    "    'review': padded_sequences,\n",
    "    'types': data['types_encoded'].values,\n",
    "}\n",
    "\n",
    "# Normalize the sentiment scores\n",
    "y = data['sentiment'].values\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T18:53:42.186883Z",
     "start_time": "2024-06-14T18:53:38.245957Z"
    }
   },
   "id": "4fce0614da3d08b4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 01:53:38.774394: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-15 01:53:39.559463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Define input layers\n",
    "review_input = Input(shape=(max_sequence_length,), name='review')\n",
    "types_input = Input(shape=(1,), name='types')\n",
    "\n",
    "# Define embedding and LSTM layers for review input\n",
    "review_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(review_input)\n",
    "review_lstm = LSTM(128)(review_embedding)\n",
    "\n",
    "# Define embedding layer for types input\n",
    "types_embedding = Embedding(input_dim=data['types_encoded'].nunique(), output_dim=10)(types_input)\n",
    "types_flat = Flatten()(types_embedding)\n",
    "\n",
    "# Concatenate the review and types embeddings\n",
    "concatenated = Concatenate()([review_lstm, types_flat])\n",
    "\n",
    "# Sequential part of the model\n",
    "sequential_model = Sequential([\n",
    "    Input(shape=(concatenated.shape[1],)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Full model combining the inputs and sequential model\n",
    "output = sequential_model(concatenated)\n",
    "full_model = Model(inputs=[review_input, types_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "full_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "full_model.fit([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "full_model.save('39_test_modelV2.keras')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2024-06-14T19:03:20.507363Z",
     "start_time": "2024-06-14T18:53:42.188326Z"
    }
   },
   "id": "16764ec0f1acbb1f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 01:53:42.392919: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-15 01:53:42.500449: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-15 01:53:42.500511: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-15 01:53:42.503346: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-15 01:53:42.503414: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-15 01:53:42.503442: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-15 01:53:42.650831: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-15 01:53:42.650897: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-15 01:53:42.650905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-15 01:53:42.650955: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-15 01:53:42.651255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinus/anaconda3/envs/capstone/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-06-15 01:53:45.131198: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 71ms/step - loss: 1.0101 - mae: 0.5668 - val_loss: 0.0912 - val_mae: 0.2280\n",
      "Epoch 2/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m56s\u001B[0m 69ms/step - loss: 0.0616 - mae: 0.1822 - val_loss: 0.0610 - val_mae: 0.1783\n",
      "Epoch 3/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m58s\u001B[0m 70ms/step - loss: 0.0331 - mae: 0.1314 - val_loss: 0.0598 - val_mae: 0.1736\n",
      "Epoch 4/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m58s\u001B[0m 71ms/step - loss: 0.0221 - mae: 0.1073 - val_loss: 0.0551 - val_mae: 0.1703\n",
      "Epoch 5/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m59s\u001B[0m 72ms/step - loss: 0.0167 - mae: 0.0929 - val_loss: 0.0388 - val_mae: 0.1322\n",
      "Epoch 6/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m59s\u001B[0m 72ms/step - loss: 0.0137 - mae: 0.0851 - val_loss: 0.0391 - val_mae: 0.1368\n",
      "Epoch 7/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m57s\u001B[0m 69ms/step - loss: 0.0112 - mae: 0.0777 - val_loss: 0.0332 - val_mae: 0.1222\n",
      "Epoch 8/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m55s\u001B[0m 67ms/step - loss: 0.0096 - mae: 0.0699 - val_loss: 0.0311 - val_mae: 0.1165\n",
      "Epoch 9/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m56s\u001B[0m 68ms/step - loss: 0.0079 - mae: 0.0644 - val_loss: 0.0311 - val_mae: 0.1233\n",
      "Epoch 10/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m59s\u001B[0m 72ms/step - loss: 0.0064 - mae: 0.0588 - val_loss: 0.0282 - val_mae: 0.1137\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T19:03:20.511448Z",
     "start_time": "2024-06-14T19:03:20.508783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class SentimentHyperModel(HyperModel):\n",
    "#     def build(self, hp):\n",
    "#         review_input = Input(shape=(max_sequence_length,), name='review')\n",
    "#         types_input = Input(shape=(1,), name='types')\n",
    "# \n",
    "#         # Define embedding and LSTM layers for review input\n",
    "#         embedding_output_dim = hp.Int('embedding_output_dim', min_value=64, max_value=256, step=32)\n",
    "#         lstm_units = hp.Int('lstm_units', min_value=64, max_value=256, step=32)\n",
    "#         review_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_output_dim)(review_input)\n",
    "#         review_lstm = LSTM(units=lstm_units)(review_embedding)\n",
    "# \n",
    "#         # Define embedding layer for types input\n",
    "#         types_embedding = Embedding(input_dim=data['types_encoded'].nunique(), output_dim=10)(types_input)\n",
    "#         types_flat = Flatten()(types_embedding)\n",
    "# \n",
    "#         # Concatenate the review and types embeddings\n",
    "#         concatenated = Concatenate()([review_lstm, types_flat])\n",
    "# \n",
    "#         # Add dense layers for final prediction\n",
    "#         dense_units_1 = hp.Int('dense_units_1', min_value=64, max_value=256, step=32)\n",
    "#         dense_units_2 = hp.Int('dense_units_2', min_value=32, max_value=128, step=16)\n",
    "#         dense_1 = Dense(units=dense_units_1, activation='relu')(concatenated)\n",
    "#         dense_2 = Dense(units=dense_units_2, activation='relu')(dense_1)\n",
    "#         output = Dense(1, activation='linear')(dense_2)\n",
    "# \n",
    "#         # Choose an optimizer\n",
    "#         optimizer_choice = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
    "# \n",
    "#         if optimizer_choice == 'adam':\n",
    "#             optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "#         elif optimizer_choice == 'sgd':\n",
    "#             optimizer = tf.keras.optimizers.SGD(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "#         elif optimizer_choice == 'rmsprop':\n",
    "#             optimizer = tf.keras.optimizers.RMSprop(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "# \n",
    "#         # Create the model\n",
    "#         model = Model(inputs=[review_input, types_input], outputs=output)\n",
    "#         model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "# \n",
    "#         return model"
   ],
   "id": "7a8333f8e323cb4b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T19:03:20.528818Z",
     "start_time": "2024-06-14T19:03:20.513659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tuner = RandomSearch(\n",
    "#     hypermodel=SentimentHyperModel(),\n",
    "#     objective='val_loss',\n",
    "#     max_trials=10,  # Number of different hyperparameter sets to try\n",
    "#     executions_per_trial=2,  # Number of models to train with the same hyperparameters\n",
    "#     directory='model-testing',\n",
    "#     project_name='sentiment_tuning'\n",
    "# )\n",
    "# \n",
    "# # %%\n",
    "# # Search for the best hyperparameters\n",
    "# tuner.search([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# \n",
    "# # Get the optimal hyperparameters\n",
    "# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "# \n",
    "# # Build the best model\n",
    "# best_model = tuner.hypermodel.build(best_hps)"
   ],
   "id": "633172108e147ab3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T19:03:20.541544Z",
     "start_time": "2024-06-14T19:03:20.530270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# best_model.fit([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# \n",
    "# # %%\n",
    "# # Save the best model\n",
    "# best_model.save('best_sentiment_model.keras')"
   ],
   "id": "2d8966f7cb395973",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T19:03:47.781723Z",
     "start_time": "2024-06-14T19:03:45.686198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('39_test_modelV2.keras')"
   ],
   "id": "8bf1e691a2787bfd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X['review'][place_idx]\n",
    "    place_types = X['types'][place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X['review'], X['types']])\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X['review'], X['types'].reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Example \n",
    "place_id = 'ChIJIaGQ-Eg60i0RnT9pzyD_gvM'  # Replace with an actual place ID from your dataset\n",
    "recommendations = get_recommendations(place_id, data, model, top_n=10)\n",
    "print(recommendations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T19:04:12.749226Z",
     "start_time": "2024-06-14T19:03:48.419850Z"
    }
   },
   "id": "ade66f54ef88bcd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1024/1024\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 23ms/step\n",
      "(                                id                                    types  \\\n",
      "2552   ChIJIaGQ-Eg60i0RnT9pzyD_gvM                               cafe, food   \n",
      "18987  ChIJjT4DJK5G0i0R3pksi46oHZY                               cafe, food   \n",
      "23697  ChIJ1dMM21FH0i0Ru3XNx9p8_S0  indonesian_restaurant, restaurant, food   \n",
      "31801  ChIJAe-Pc09F0i0RFm0SEDsDyU8                       tourist_attraction   \n",
      "13981  ChIJxdi5l84n0i0RyUyMbwuSf1w                         restaurant, food   \n",
      "20320  ChIJqeoHnm2H0S0Re7c7kU8NUtE                 park, tourist_attraction   \n",
      "2716   ChIJ65f5180V0i0RkMx79fIo0Ts                         restaurant, food   \n",
      "4039   ChIJz0XREtZH0i0RowHdlImQUYQ           coffee_shop, cafe, store, food   \n",
      "30428  ChIJQ7sXNoZB0i0RLRxhrTl5500                           hotel, lodging   \n",
      "31575  ChIJt_0lSYZz0i0RfM-BdC8kMhU                         restaurant, food   \n",
      "\n",
      "      review_number                                             review  \\\n",
      "2552       review 1  Wajib mampir,, matcha milk tea with pearl, my ...   \n",
      "18987      review 3  Recommended! The food tastes great. Wifi and e...   \n",
      "23697      review 4  Some tasty pork dishes with an indo flavour at...   \n",
      "31801      review 5  Good beach, so many fisherman ship on here. Yo...   \n",
      "13981      review 3  Best restaurant around jati luwih area , nice ...   \n",
      "20320      review 4  Keep on walking through some beautiful Cacao, ...   \n",
      "2716       review 1  I can confirm with the other 1 star review, ca...   \n",
      "4039       review 1  Japanese ice coffee tastes good n the cookies ...   \n",
      "30428      review 5  The room was nice and clean. The bathroom need...   \n",
      "31575      review 5  It was sooo delicious! If you want to try auth...   \n",
      "\n",
      "          user_id  sentiment  types_encoded  \n",
      "2552   user_37983       5.00            153  \n",
      "18987  user_43617       4.47            153  \n",
      "23697   user_4182       4.40            469  \n",
      "31801  user_12555       4.20            860  \n",
      "13981  user_51302       3.91            719  \n",
      "20320  user_32594       4.25            557  \n",
      "2716   user_39344       3.21            719  \n",
      "4039     user_620       3.48            229  \n",
      "30428  user_27077       3.72            381  \n",
      "31575  user_30206       3.88            719  , array([[5.1006775],\n",
      "       [4.371896 ],\n",
      "       [4.546817 ],\n",
      "       [4.456439 ],\n",
      "       [3.9191284],\n",
      "       [4.2181754],\n",
      "       [3.15923  ],\n",
      "       [3.5911217],\n",
      "       [3.9123974],\n",
      "       [3.920897 ]], dtype=float32))\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "model.save('39_test_model.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c28122d45aa642c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "# model = tf.keras.models.load_model('39_test_model.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f8b901fff697f59",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place :{rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "recommendations = get_recommendations(rand_id, df_review, model, top_n=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73f3d0d247699c85",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "# sort reccomendations by sentiment\n",
    "sorted_reccomendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "# Print the recommendations with place names with out rand_ind\n",
    "print(sorted_reccomendations[['name','types_x', 'rating']])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6dc751da34a7a4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Test cell, Run this cell to get recommendations for a random place in the dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Flatten\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Create the feature set\n",
    "    X = {\n",
    "        'review': padded_sequences,\n",
    "        'types': data['types_encoded'].values,\n",
    "    }\n",
    "    \n",
    "    # Normalize the sentiment scores\n",
    "    y = data['sentiment'].values\n",
    "\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X['review'][place_idx]\n",
    "    place_types = X['types'][place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X['review'], X['types']])\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X['review'], X['types'].reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# model = tf.keras.models.load_model('best_sentiment_model.keras')\n",
    "model = tf.keras.models.load_model('39_test_model.keras')\n",
    "\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place :{rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "recommendations = get_recommendations(rand_id, df_review, model, top_n=10)\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "# sort reccomendations by sentiment\n",
    "sorted_reccomendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "# Print the recommendations with place names with out rand_ind\n",
    "print(sorted_reccomendations[['name','types_x', 'rating']])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "774efa3b0ffded2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a3e19ef7827c63e2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
