{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Flatten\n",
    "from keras_tuner import RandomSearch, HyperModel\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'combined-dataset/final_reviews_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode the 'types' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "\n",
    "# Tokenize the 'review' column\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "\n",
    "# Pad the sequences\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create the feature set\n",
    "X = {\n",
    "    'review': padded_sequences,\n",
    "    'types': data['types_encoded'].values,\n",
    "}\n",
    "\n",
    "# Normalize the sentiment scores\n",
    "y = data['sentiment'].values\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fce0614da3d08b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# # Define input layers\n",
    "# review_input = Input(shape=(max_sequence_length,), name='review')\n",
    "# types_input = Input(shape=(1,), name='types')\n",
    "# \n",
    "# # Define embedding and LSTM layers for review input\n",
    "# review_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(review_input)\n",
    "# review_lstm = LSTM(128)(review_embedding)\n",
    "# \n",
    "# # Define embedding layer for types input\n",
    "# types_embedding = Embedding(input_dim=data['types_encoded'].nunique(), output_dim=10)(types_input)\n",
    "# types_flat = tf.keras.layers.Flatten()(types_embedding)\n",
    "# \n",
    "# # Concatenate the review and types embeddings\n",
    "# concatenated = Concatenate()([review_lstm, types_flat])\n",
    "# \n",
    "# # Add dense layers for final prediction\n",
    "# dense_1 = Dense(128, activation='relu')(concatenated)\n",
    "# dense_2 = Dense(64, activation='relu')(dense_1)\n",
    "# output = Dense(1, activation='linear')(dense_2)\n",
    "# \n",
    "# # Create the model\n",
    "# model = Model(inputs=[review_input, types_input], outputs=output)\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "# \n",
    "# # Train the model\n",
    "# model.fit([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16764ec0f1acbb1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SentimentHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        review_input = Input(shape=(max_sequence_length,), name='review')\n",
    "        types_input = Input(shape=(1,), name='types')\n",
    "\n",
    "        # Define embedding and LSTM layers for review input\n",
    "        embedding_output_dim = hp.Int('embedding_output_dim', min_value=64, max_value=256, step=32)\n",
    "        lstm_units = hp.Int('lstm_units', min_value=64, max_value=256, step=32)\n",
    "        review_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_output_dim)(review_input)\n",
    "        review_lstm = LSTM(units=lstm_units)(review_embedding)\n",
    "\n",
    "        # Define embedding layer for types input\n",
    "        types_embedding = Embedding(input_dim=data['types_encoded'].nunique(), output_dim=10)(types_input)\n",
    "        types_flat = Flatten()(types_embedding)\n",
    "\n",
    "        # Concatenate the review and types embeddings\n",
    "        concatenated = Concatenate()([review_lstm, types_flat])\n",
    "\n",
    "        # Add dense layers for final prediction\n",
    "        dense_units_1 = hp.Int('dense_units_1', min_value=64, max_value=256, step=32)\n",
    "        dense_units_2 = hp.Int('dense_units_2', min_value=32, max_value=128, step=16)\n",
    "        dense_1 = Dense(units=dense_units_1, activation='relu')(concatenated)\n",
    "        dense_2 = Dense(units=dense_units_2, activation='relu')(dense_1)\n",
    "        output = Dense(1, activation='linear')(dense_2)\n",
    "\n",
    "        # Choose an optimizer\n",
    "        optimizer_choice = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
    "\n",
    "        if optimizer_choice == 'adam':\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "        elif optimizer_choice == 'sgd':\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "        elif optimizer_choice == 'rmsprop':\n",
    "            optimizer = tf.keras.optimizers.RMSprop(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "\n",
    "        # Create the model\n",
    "        model = Model(inputs=[review_input, types_input], outputs=output)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "        return model"
   ],
   "id": "7a8333f8e323cb4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tuner = RandomSearch(\n",
    "    hypermodel=SentimentHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # Number of different hyperparameter sets to try\n",
    "    executions_per_trial=2,  # Number of models to train with the same hyperparameters\n",
    "    directory='model-testing',\n",
    "    project_name='sentiment_tuning'\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Search for the best hyperparameters\n",
    "tuner.search([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the best model\n",
    "best_model = tuner.hypermodel.build(best_hps)"
   ],
   "id": "633172108e147ab3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_model.fit([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# %%\n",
    "# Save the best model\n",
    "best_model.save('best_sentiment_model.keras')"
   ],
   "id": "2d8966f7cb395973",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = tf.keras.models.load_model('best_sentiment_model.keras')",
   "id": "8bf1e691a2787bfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X['review'][place_idx]\n",
    "    place_types = X['types'][place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X['review'], X['types']])\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X['review'], X['types'].reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places\n",
    "\n",
    "# Example usage\n",
    "place_id = 'ChIJYcGr7GSb0S0RckePBrCWikw'  # Replace with an actual place ID from your dataset\n",
    "recommendations = get_recommendations(place_id, data, model, top_n=10)\n",
    "print(recommendations)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ade66f54ef88bcd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "# model.save('39_test_model.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c28122d45aa642c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "# model = tf.keras.models.load_model('39_test_model.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f8b901fff697f59",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place :{rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "recommendations = get_recommendations(rand_id, df_review, model, top_n=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73f3d0d247699c85",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "# sort reccomendations by sentiment\n",
    "sorted_reccomendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "# Print the recommendations with place names with out rand_ind\n",
    "print(sorted_reccomendations[['name','types_x', 'rating']])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6dc751da34a7a4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "774efa3b0ffded2d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
