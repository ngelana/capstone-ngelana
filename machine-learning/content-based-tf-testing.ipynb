{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'combined-dataset/final_reviews_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode the 'types' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "\n",
    "# Tokenize the 'review' column\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "\n",
    "# Pad the sequences\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create the feature set\n",
    "X = {\n",
    "    'review': padded_sequences,\n",
    "    'types': data['types_encoded'].values,\n",
    "}\n",
    "\n",
    "# Normalize the sentiment scores\n",
    "y = data['sentiment'].values\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:16:51.098925800Z",
     "start_time": "2024-06-12T15:16:47.790981100Z"
    }
   },
   "id": "4fce0614da3d08b4",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m819/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m547s\u001B[0m 665ms/step - loss: 0.8965 - mae: 0.5329 - val_loss: 0.0898 - val_mae: 0.2247\n",
      "Epoch 2/10\n",
      "\u001B[1m592/819\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m2:32\u001B[0m 670ms/step - loss: 0.0533 - mae: 0.1678"
     ]
    }
   ],
   "source": [
    "# Define input layers\n",
    "review_input = Input(shape=(max_sequence_length,), name='review')\n",
    "types_input = Input(shape=(1,), name='types')\n",
    "\n",
    "# Define embedding and LSTM layers for review input\n",
    "review_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(review_input)\n",
    "review_lstm = LSTM(128)(review_embedding)\n",
    "\n",
    "# Define embedding layer for types input\n",
    "types_embedding = Embedding(input_dim=data['types_encoded'].nunique(), output_dim=10)(types_input)\n",
    "types_flat = tf.keras.layers.Flatten()(types_embedding)\n",
    "\n",
    "# Concatenate the review and types embeddings\n",
    "concatenated = Concatenate()([review_lstm, types_flat])\n",
    "\n",
    "# Add dense layers for final prediction\n",
    "dense_1 = Dense(128, activation='relu')(concatenated)\n",
    "dense_2 = Dense(64, activation='relu')(dense_1)\n",
    "output = Dense(1, activation='linear')(dense_2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[review_input, types_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-12T15:16:51.096924800Z"
    }
   },
   "id": "16764ec0f1acbb1f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(32745, 779)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X['review'][place_idx]\n",
    "    place_types = X['types'][place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X['review'], X['types']])\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X['review'], X['types'].reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places\n",
    "\n",
    "# Example usage\n",
    "place_id = 'ChIJYcGr7GSb0S0RckePBrCWikw'  # Replace with an actual place ID from your dataset\n",
    "recommendations = get_recommendations(place_id, data, model, top_n=10)\n",
    "print(recommendations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T14:58:55.751968900Z",
     "start_time": "2024-06-12T14:58:52.858364100Z"
    }
   },
   "id": "ade66f54ef88bcd5",
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
