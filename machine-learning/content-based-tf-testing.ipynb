{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# from keras_tuner import RandomSearch, HyperModel\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'combined-dataset/final_reviews_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode the 'types' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "\n",
    "# Tokenize the 'review' column\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "\n",
    "# Pad the sequences\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create the feature set\n",
    "X = {\n",
    "    'review': padded_sequences,\n",
    "    'types': data['types_encoded'].values,\n",
    "}\n",
    "\n",
    "# Normalize the sentiment scores\n",
    "y = data['sentiment'].values\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T15:03:01.951749Z",
     "start_time": "2024-06-15T15:02:57.891300Z"
    }
   },
   "id": "4fce0614da3d08b4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 22:02:58.386681: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-15 22:02:58.386749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-15 22:02:58.387338: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-15 22:02:58.391827: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-15 22:02:59.143868: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T15:12:04.791644Z",
     "start_time": "2024-06-15T15:03:16.541192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define input layers\n",
    "review_input = Input(shape=(max_sequence_length,), name='review')\n",
    "types_input = Input(shape=(1,), name='types')\n",
    "\n",
    "# Define embedding and LSTM layers for review input\n",
    "review_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(review_input)\n",
    "review_lstm = LSTM(128)(review_embedding)\n",
    "\n",
    "# Define embedding layer for types input\n",
    "types_embedding = Embedding(input_dim=data['types_encoded'].nunique(), output_dim=10)(types_input)\n",
    "types_flat = Flatten()(types_embedding)\n",
    "\n",
    "# Concatenate the review and types embeddings\n",
    "concatenated = Concatenate()([review_lstm, types_flat])\n",
    "\n",
    "# Sequential part of the model\n",
    "sequential_model = Sequential([\n",
    "    Input(shape=(concatenated.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Full model combining the inputs and sequential model\n",
    "output = sequential_model(concatenated)\n",
    "full_model = Model(inputs=[review_input, types_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "full_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "full_model.fit([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "full_model.save('test.keras')"
   ],
   "id": "c94240a53f1217d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "819/819 [==============================] - 70s 83ms/step - loss: 0.3071 - mae: 0.3408 - val_loss: 0.1279 - val_mae: 0.2691\n",
      "Epoch 2/10\n",
      "819/819 [==============================] - 53s 65ms/step - loss: 0.0527 - mae: 0.1631 - val_loss: 0.0558 - val_mae: 0.1673\n",
      "Epoch 3/10\n",
      "819/819 [==============================] - 49s 60ms/step - loss: 0.0259 - mae: 0.1119 - val_loss: 0.0395 - val_mae: 0.1326\n",
      "Epoch 4/10\n",
      "819/819 [==============================] - 48s 59ms/step - loss: 0.0149 - mae: 0.0842 - val_loss: 0.0395 - val_mae: 0.1314\n",
      "Epoch 5/10\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 0.0110 - mae: 0.0727 - val_loss: 0.0404 - val_mae: 0.1412\n",
      "Epoch 6/10\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 0.0085 - mae: 0.0639 - val_loss: 0.0308 - val_mae: 0.1184\n",
      "Epoch 7/10\n",
      "819/819 [==============================] - 51s 63ms/step - loss: 0.0072 - mae: 0.0591 - val_loss: 0.0298 - val_mae: 0.1177\n",
      "Epoch 8/10\n",
      "819/819 [==============================] - 51s 63ms/step - loss: 0.0062 - mae: 0.0556 - val_loss: 0.0272 - val_mae: 0.1101\n",
      "Epoch 9/10\n",
      "819/819 [==============================] - 53s 65ms/step - loss: 0.0057 - mae: 0.0534 - val_loss: 0.0289 - val_mae: 0.1191\n",
      "Epoch 10/10\n",
      "819/819 [==============================] - 52s 63ms/step - loss: 0.0050 - mae: 0.0502 - val_loss: 0.0252 - val_mae: 0.1023\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T15:27:53.515214Z",
     "start_time": "2024-06-15T15:27:50.846029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('test.keras')"
   ],
   "id": "633172108e147ab3",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer 'embedding_2' expected 1 variables, but received 0 variables during loading. Expected: ['embedding_2/embeddings:0']",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m39_test_modelV3.keras\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/testing/lib/python3.11/site-packages/keras/src/saving/saving_api.py:254\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001B[0m\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m kwargs:\n\u001B[1;32m    250\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    251\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe following argument(s) are not supported \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    252\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwith the native Keras format: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(kwargs\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    253\u001B[0m         )\n\u001B[0;32m--> 254\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msaving_lib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcustom_objects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcustom_objects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcompile\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mcompile\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m        \u001B[49m\u001B[43msafe_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msafe_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;66;03m# Legacy case.\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m legacy_sm_saving_lib\u001B[38;5;241m.\u001B[39mload_model(\n\u001B[1;32m    263\u001B[0m     filepath, custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects, \u001B[38;5;28mcompile\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcompile\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    264\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/testing/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:281\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(filepath, custom_objects, compile, safe_mode)\u001B[0m\n\u001B[1;32m    278\u001B[0m             asset_store\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    283\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/anaconda3/envs/testing/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:269\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(filepath, custom_objects, compile, safe_mode)\u001B[0m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    267\u001B[0m     asset_store \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 269\u001B[0m \u001B[43m_load_state\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweights_store\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights_store\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43massets_store\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43masset_store\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43minner_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvisited_trackables\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mset\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    276\u001B[0m weights_store\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m asset_store:\n",
      "File \u001B[0;32m~/anaconda3/envs/testing/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:466\u001B[0m, in \u001B[0;36m_load_state\u001B[0;34m(trackable, weights_store, assets_store, inner_path, skip_mismatch, visited_trackables)\u001B[0m\n\u001B[1;32m    457\u001B[0m     _load_state(\n\u001B[1;32m    458\u001B[0m         child_obj,\n\u001B[1;32m    459\u001B[0m         weights_store,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    463\u001B[0m         visited_trackables\u001B[38;5;241m=\u001B[39mvisited_trackables,\n\u001B[1;32m    464\u001B[0m     )\n\u001B[1;32m    465\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(child_obj, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mdict\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mset\u001B[39m)):\n\u001B[0;32m--> 466\u001B[0m     \u001B[43m_load_container_state\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchild_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    468\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweights_store\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    469\u001B[0m \u001B[43m        \u001B[49m\u001B[43massets_store\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    470\u001B[0m \u001B[43m        \u001B[49m\u001B[43minner_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43minner_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchild_attr\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    471\u001B[0m \u001B[43m        \u001B[49m\u001B[43mskip_mismatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_mismatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    472\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvisited_trackables\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvisited_trackables\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/testing/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:534\u001B[0m, in \u001B[0;36m_load_container_state\u001B[0;34m(container, weights_store, assets_store, inner_path, skip_mismatch, visited_trackables)\u001B[0m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    533\u001B[0m     used_names[name] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 534\u001B[0m \u001B[43m_load_state\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    535\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrackable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    536\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweights_store\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    537\u001B[0m \u001B[43m    \u001B[49m\u001B[43massets_store\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    538\u001B[0m \u001B[43m    \u001B[49m\u001B[43minner_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43minner_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    539\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_mismatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_mismatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvisited_trackables\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvisited_trackables\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    541\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/testing/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:435\u001B[0m, in \u001B[0;36m_load_state\u001B[0;34m(trackable, weights_store, assets_store, inner_path, skip_mismatch, visited_trackables)\u001B[0m\n\u001B[1;32m    428\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    429\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not load weights in object \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrackable\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    430\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSkipping object. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    431\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mException encountered: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    432\u001B[0m                 stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m    433\u001B[0m             )\n\u001B[1;32m    434\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 435\u001B[0m         \u001B[43mtrackable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_own_variables\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights_store\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43minner_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(trackable, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mload_assets\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m assets_store:\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m skip_mismatch:\n",
      "File \u001B[0;32m~/anaconda3/envs/testing/lib/python3.11/site-packages/keras/src/engine/base_layer.py:3531\u001B[0m, in \u001B[0;36mLayer.load_own_variables\u001B[0;34m(self, store)\u001B[0m\n\u001B[1;32m   3529\u001B[0m all_vars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_trainable_weights \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_non_trainable_weights\n\u001B[1;32m   3530\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(store\u001B[38;5;241m.\u001B[39mkeys()) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(all_vars):\n\u001B[0;32m-> 3531\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   3532\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLayer \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m expected \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(all_vars)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m variables, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3533\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut received \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3534\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(store\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m variables during loading. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3535\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m[v\u001B[38;5;241m.\u001B[39mname\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mv\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mall_vars]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3536\u001B[0m     )\n\u001B[1;32m   3537\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(all_vars):\n\u001B[1;32m   3538\u001B[0m     \u001B[38;5;66;03m# TODO(rchao): check shapes and raise errors.\u001B[39;00m\n\u001B[1;32m   3539\u001B[0m     v\u001B[38;5;241m.\u001B[39massign(store[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m])\n",
      "\u001B[0;31mValueError\u001B[0m: Layer 'embedding_2' expected 1 variables, but received 0 variables during loading. Expected: ['embedding_2/embeddings:0']"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X['review'][place_idx]\n",
    "    place_types = X['types'][place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X['review'], X['types']])\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X['review'], X['types'].reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Example \n",
    "place_id = 'ChIJIaGQ-Eg60i0RnT9pzyD_gvM'  # Replace with an actual place ID from your dataset\n",
    "recommendations = get_recommendations(place_id, data, model, top_n=10)\n",
    "print(recommendations)\n"
   ],
   "id": "2d8966f7cb395973",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "model.save('39_test_model.keras')"
   ],
   "id": "8bf1e691a2787bfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "# model = tf.keras.models.load_model('39_test_model.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ade66f54ef88bcd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place :{rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "recommendations = get_recommendations(rand_id, df_review, model, top_n=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c28122d45aa642c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "# sort reccomendations by sentiment\n",
    "sorted_reccomendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "# Print the recommendations with place names with out rand_ind\n",
    "print(sorted_reccomendations[['name','types_x', 'rating']])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f8b901fff697f59",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Test cell, Run this cell to get recommendations for a random place in the dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Flatten\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Create the feature set\n",
    "    X = {\n",
    "        'review': padded_sequences,\n",
    "        'types': data['types_encoded'].values,\n",
    "    }\n",
    "    \n",
    "    # Normalize the sentiment scores\n",
    "    y = data['sentiment'].values\n",
    "\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X['review'][place_idx]\n",
    "    place_types = X['types'][place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X['review'], X['types']])\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X['review'], X['types'].reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# model = tf.keras.models.load_model('best_sentiment_model.keras')\n",
    "model = tf.keras.models.load_model('39_test_modelV3.keras')\n",
    "\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place :{rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "recommendations = get_recommendations('ChIJQ5jInls_0i0Ra53iWVquuq8', df_review, model, top_n=10)[0]\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "# sort reccomendations by sentiment\n",
    "sorted_reccomendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "# Print the recommendations with place names with out rand_ind\n",
    "print(sorted_reccomendations[['name','types_x', 'rating']])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73f3d0d247699c85",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    # Encode the 'types' column\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Prepare the input features\n",
    "    X_review = padded_sequences\n",
    "    X_types = data['types_encoded'].values\n",
    "\n",
    "    # Get the index of the specified place_id\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X_review[place_idx]\n",
    "    place_types = X_types[place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X_review, X_types], batch_size=128, verbose=0)\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X_review, X_types.reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n-1:][::-1]\n",
    "    similar_indices = similar_indices[similar_indices != place_idx][:top_n]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('39_test_modelV3.keras')\n",
    "\n",
    "# Load the datasets\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "# Get a random place ID\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place: {rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "# Get recommendations\n",
    "recommendations, predicted_sentiments = get_recommendations('ChIJQ5jInls_0i0Ra53iWVquuq8', df_review, model, top_n=10)\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "\n",
    "# Sort recommendations by sentiment\n",
    "sorted_recommendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "\n",
    "# Print the recommendations with place names without rand_id\n",
    "print(sorted_recommendations[['name', 'types_x', 'rating']])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6dc751da34a7a4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import faiss\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    # Encode the 'types' column\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Prepare the input features\n",
    "    X_review = padded_sequences\n",
    "    X_types = data['types_encoded'].values\n",
    "\n",
    "    # Get the index of the specified place_id\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X_review[place_idx]\n",
    "    place_types = X_types[place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X_review, X_types], batch_size=128, verbose=0)\n",
    "\n",
    "    # Combine review and types vectors\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X_review, X_types.reshape(-1, 1)])\n",
    "\n",
    "    # Using Faiss for approximate nearest neighbors\n",
    "    d = all_vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(all_vectors.astype(np.float32))\n",
    "    D, I = index.search(np.array([place_vector.astype(np.float32)]), top_n + 1)\n",
    "\n",
    "    # Get top N similar places (excluding the place itself)\n",
    "    similar_indices = I[0][I[0] != place_idx][:top_n]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('39_test_modelV3.keras')\n",
    "\n",
    "# Load the datasets\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "# Get a random place ID\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place: {rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "# Get recommendations\n",
    "recommendations, predicted_sentiments = get_recommendations('ChIJQ5jInls_0i0Ra53iWVquuq8', df_review, model, top_n=10)\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "\n",
    "# Sort recommendations by sentiment\n",
    "sorted_recommendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "\n",
    "# Print the recommendations with place names without rand_id\n",
    "print(sorted_recommendations[['name', 'types_x', 'rating']])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "774efa3b0ffded2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T10:03:36.713911Z",
     "start_time": "2024-06-15T10:03:36.711555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import faiss\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    # Encode the 'types' column\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Prepare the input features\n",
    "    X_review = padded_sequences\n",
    "    X_types = data['types_encoded'].values.reshape(-1, 1)\n",
    "\n",
    "    # Combine review and types vectors\n",
    "    combined_vectors = np.hstack([X_review, X_types])\n",
    "\n",
    "    # Dimensionality reduction using PCA\n",
    "    pca = PCA(n_components=50)  # Adjust the number of components as needed\n",
    "    reduced_vectors = pca.fit_transform(combined_vectors)\n",
    "\n",
    "    # Get the index of the specified place_id\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_vector = reduced_vectors[place_idx]\n",
    "\n",
    "    # Using Faiss for approximate nearest neighbors\n",
    "    d = reduced_vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(reduced_vectors.astype(np.float32))\n",
    "    D, I = index.search(np.array([place_vector.astype(np.float32)]), top_n + 1)\n",
    "\n",
    "    # Get top N similar places (excluding the place itself)\n",
    "    similar_indices = I[0][I[0] != place_idx][:top_n]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X_review, X_types.squeeze()], batch_size=128, verbose=0)\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('39_test_modelV3.keras')\n",
    "\n",
    "# Load the datasets\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "# Get a random place ID\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place: {rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "# Get recommendations\n",
    "recommendations, predicted_sentiments = get_recommendations('ChIJQ5jInls_0i0Ra53iWVquuq8', df_review, model, top_n=10)\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "\n",
    "# Sort recommendations by sentiment\n",
    "sorted_recommendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "\n",
    "# Print the recommendations with place names without rand_id\n",
    "print(sorted_recommendations[['name', 'types_x', 'rating']])\n"
   ],
   "id": "3e7fb3996cef3c51",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T10:03:38.054866Z",
     "start_time": "2024-06-15T10:03:37.678848Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e188b7c058d0248b",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_ort' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 28\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Example \u001B[39;00m\n\u001B[1;32m     27\u001B[0m place_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mChIJIaGQ-Eg60i0RnT9pzyD_gvM\u001B[39m\u001B[38;5;124m'\u001B[39m  \u001B[38;5;66;03m# Replace with an actual place ID from your dataset\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m recommendations \u001B[38;5;241m=\u001B[39m \u001B[43mget_recommendations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mplace_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_n\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(recommendations)\n",
      "Cell \u001B[0;32mIn[10], line 24\u001B[0m, in \u001B[0;36mget_recommendations\u001B[0;34m(place_id, data, top_n)\u001B[0m\n\u001B[1;32m     21\u001B[0m similar_indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margsort(similarities)[\u001B[38;5;241m-\u001B[39mtop_n:][::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     22\u001B[0m similar_places \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39miloc[similar_indices]\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m similar_places, \u001B[43mresults_ort\u001B[49m[\u001B[38;5;241m0\u001B[39m][similar_indices]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'results_ort' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T10:15:29.566471Z",
     "start_time": "2024-06-15T10:15:29.484374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession('model.onnx', providers=['CUDAExecutionProvider'])"
   ],
   "id": "a3e19ef7827c63e2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2024-06-15 17:15:29.548250196 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001B[m\n",
      "\u001B[0;93m2024-06-15 17:15:29.548294410 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001B[m\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T10:15:30.211979Z",
     "start_time": "2024-06-15T10:15:30.169954Z"
    }
   },
   "cell_type": "code",
   "source": "results_ort = session.run(None, {\"types\": X['types'].reshape(-1, 1), \"review\": X['review'].astype(np.float32)})",
   "id": "22dbf1dcb30dc0e2",
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(int64)) , expected: (tensor(float))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgument\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m results_ort \u001B[38;5;241m=\u001B[39m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtypes\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtypes\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreview\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mreview\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/testing/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001B[0m, in \u001B[0;36mSession.run\u001B[0;34m(self, output_names, input_feed, run_options)\u001B[0m\n\u001B[1;32m    218\u001B[0m     output_names \u001B[38;5;241m=\u001B[39m [output\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs_meta]\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 220\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_feed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m C\u001B[38;5;241m.\u001B[39mEPFail \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    222\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_fallback:\n",
      "\u001B[0;31mInvalidArgument\u001B[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (tensor(int64)) , expected: (tensor(float))"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T10:08:18.276947Z",
     "start_time": "2024-06-15T10:08:18.274082Z"
    }
   },
   "cell_type": "code",
   "source": "print(type(X['types']))",
   "id": "8b41325f3b9850a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T10:08:24.434906Z",
     "start_time": "2024-06-15T10:08:24.430552Z"
    }
   },
   "cell_type": "code",
   "source": "X['types']",
   "id": "cf21a265d88934af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([381, 469, 896, ..., 860,  59, 560])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "12ea25b9e40ff802"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
