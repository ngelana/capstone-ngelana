{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# from keras_tuner import RandomSearch, HyperModel\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'combined-dataset/final_reviews_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode the 'types' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "\n",
    "# Tokenize the 'review' column\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "\n",
    "# Pad the sequences\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create the feature set\n",
    "X = {\n",
    "    'review': padded_sequences,\n",
    "    'types': data['types_encoded'].values,\n",
    "}\n",
    "\n",
    "# Normalize the sentiment scores\n",
    "y = data['sentiment'].values\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T12:22:49.802192Z",
     "start_time": "2024-06-17T12:22:46.217454600Z"
    }
   },
   "id": "4fce0614da3d08b4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define input layers\n",
    "review_input = Input(shape=(max_sequence_length,), name='review')\n",
    "types_input = Input(shape=(1,), name='types')\n",
    "\n",
    "# Define embedding and LSTM layers for review input\n",
    "review_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(review_input)\n",
    "review_lstm = LSTM(128)(review_embedding)\n",
    "\n",
    "# Define embedding layer for types input\n",
    "types_embedding = Embedding(input_dim=data['types_encoded'].nunique(), output_dim=10)(types_input)\n",
    "types_flat = Flatten()(types_embedding)\n",
    "\n",
    "# Concatenate the review and types embeddings\n",
    "concatenated = Concatenate()([review_lstm, types_flat])\n",
    "\n",
    "# Sequential part of the model\n",
    "sequential_model = Sequential([\n",
    "    Input(shape=(concatenated.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Full model combining the inputs and sequential model\n",
    "output = sequential_model(concatenated)\n",
    "full_model = Model(inputs=[review_input, types_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "full_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "full_model.fit([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "full_model.save('test.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a32471ea0c7ccc83"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('39_test_modelV4.keras')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T12:23:35.806213300Z",
     "start_time": "2024-06-17T12:23:24.805351700Z"
    }
   },
   "id": "ca37420996a87f61",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 19:23:37.148427: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024/1024 [==============================] - 27s 24ms/step\n",
      "(                                id  \\\n",
      "2552   ChIJIaGQ-Eg60i0RnT9pzyD_gvM   \n",
      "18987  ChIJjT4DJK5G0i0R3pksi46oHZY   \n",
      "31801  ChIJAe-Pc09F0i0RFm0SEDsDyU8   \n",
      "13981  ChIJxdi5l84n0i0RyUyMbwuSf1w   \n",
      "20320  ChIJqeoHnm2H0S0Re7c7kU8NUtE   \n",
      "2716   ChIJ65f5180V0i0RkMx79fIo0Ts   \n",
      "31575  ChIJt_0lSYZz0i0RfM-BdC8kMhU   \n",
      "30428  ChIJQ7sXNoZB0i0RLRxhrTl5500   \n",
      "16520  ChIJh_J43ZI40i0RSVe_79mX-BM   \n",
      "26908  ChIJIyxSdyFi0S0RouWBS4YwZtw   \n",
      "\n",
      "                                              types review_number  \\\n",
      "2552                                     cafe, food      review 1   \n",
      "18987                                    cafe, food      review 3   \n",
      "31801                            tourist_attraction      review 5   \n",
      "13981                              restaurant, food      review 3   \n",
      "20320                      park, tourist_attraction      review 4   \n",
      "2716                               restaurant, food      review 1   \n",
      "31575                              restaurant, food      review 5   \n",
      "30428                                hotel, lodging      review 5   \n",
      "16520  tourist_attraction, church, place_of_worship      review 3   \n",
      "26908       indonesian_restaurant, restaurant, food      review 5   \n",
      "\n",
      "                                                  review     user_id  \\\n",
      "2552   Wajib mampir,, matcha milk tea with pearl, my ...  user_37983   \n",
      "18987  Recommended! The food tastes great. Wifi and e...  user_43617   \n",
      "31801  Good beach, so many fisherman ship on here. Yo...  user_12555   \n",
      "13981  Best restaurant around jati luwih area , nice ...  user_51302   \n",
      "20320  Keep on walking through some beautiful Cacao, ...  user_32594   \n",
      "2716   I can confirm with the other 1 star review, ca...  user_39344   \n",
      "31575  It was sooo delicious! If you want to try auth...  user_30206   \n",
      "30428  The room was nice and clean. The bathroom need...  user_27077   \n",
      "16520  Ethnic church, not to big but comfort. Easy ac...  user_60970   \n",
      "26908  A nice open air restaurant with lake and good ...  user_21320   \n",
      "\n",
      "       sentiment  types_encoded  \n",
      "2552        5.00            153  \n",
      "18987       4.47            153  \n",
      "31801       4.20            860  \n",
      "13981       3.91            719  \n",
      "20320       4.25            557  \n",
      "2716        3.21            719  \n",
      "31575       3.88            719  \n",
      "30428       3.72            381  \n",
      "16520       3.40            866  \n",
      "26908       3.56            469  , array([[4.904431 ],\n",
      "       [4.5783763],\n",
      "       [4.316544 ],\n",
      "       [3.9635773],\n",
      "       [4.400086 ],\n",
      "       [3.2056274],\n",
      "       [3.9384446],\n",
      "       [3.879598 ],\n",
      "       [3.4201233],\n",
      "       [3.6584089]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X['review'][place_idx]\n",
    "    place_types = X['types'][place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X['review'], X['types']])\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X['review'], X['types'].reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Example \n",
    "place_id = 'ChIJIaGQ-Eg60i0RnT9pzyD_gvM'  # Replace with an actual place ID from your dataset\n",
    "recommendations = get_recommendations(place_id, data, model, top_n=10)\n",
    "print(recommendations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T12:24:03.774309500Z",
     "start_time": "2024-06-17T12:23:35.806213300Z"
    }
   },
   "id": "d2782f2a9f92e810",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('39_test_model.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed1c303aca2cea11"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# model = tf.keras.models.load_model('39_test_model.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a17ae36503e3b263"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place :{rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "recommendations = get_recommendations(rand_id, df_review, model, top_n=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6d38fc873b31103"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "# sort reccomendations by sentiment\n",
    "sorted_reccomendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "# Print the recommendations with place names with out rand_ind\n",
    "print(sorted_reccomendations[['name','types_x', 'rating']])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40fe03c2a1edceaa"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n",
      "len of df 32745\n",
      "Random place :ChIJ4e5z3slB0i0RD9knyHb6U6E TAN-PANAMA COFFEE\n",
      "len of X feautre set 32745\n",
      "shape of X feautre set (32745, 779)\n",
      "1024/1024 [==============================] - 25s 23ms/step\n",
      "                                   name  \\\n",
      "6                       House of hobbit   \n",
      "2                           Nusa Penida   \n",
      "0                     TAN-PANAMA COFFEE   \n",
      "1                      Wild Habit Pizza   \n",
      "7  Oribinal Burger (by Ketumbar Studio)   \n",
      "9               25:PM Coffee - Nusa Dua   \n",
      "5                           Ampik Batur   \n",
      "4                     Damuh Guest House   \n",
      "3   IBB Waroeng ( Ikan Bakar Buleleng )   \n",
      "8                  Vrindavan Ubud Villa   \n",
      "\n",
      "                                             types_x  rating  \n",
      "6                                            lodging     4.4  \n",
      "2                                     hotel, lodging     4.9  \n",
      "0                     coffee_shop, cafe, store, food     4.6  \n",
      "1  pizza_restaurant, italian_restaurant, restaura...     4.8  \n",
      "7  hamburger_restaurant, american_restaurant, res...     4.9  \n",
      "9                     coffee_shop, cafe, store, food     4.6  \n",
      "5                                            lodging     4.4  \n",
      "4                                     hotel, lodging     4.7  \n",
      "3            indonesian_restaurant, restaurant, food     4.3  \n",
      "8                       resort_hotel, hotel, lodging     4.5  \n"
     ]
    }
   ],
   "source": [
    "# Test cell, Run this cell to get recommendations for a random place in the dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Flatten\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Create the feature set\n",
    "    X = {\n",
    "        'review': padded_sequences,\n",
    "        'types': data['types_encoded'].values,\n",
    "    }\n",
    "    print('len of X feautre set',len(X['review']))\n",
    "    print('shape of X feautre set', X['review'].shape)\n",
    "    \n",
    "    # Normalize the sentiment scores\n",
    "    y = data['sentiment'].values\n",
    "\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X['review'][place_idx]\n",
    "    place_types = X['types'][place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X['review'], X['types']])\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X['review'], X['types'].reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# model = tf.keras.models.load_model('best_sentiment_model.keras')\n",
    "model = tf.keras.models.load_model('39_test_modelV4.keras')\n",
    "\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_datav2.csv')\n",
    "print('len of df', len(df_review))\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place :{rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "recommendations = get_recommendations(rand_id, df_review, model, top_n=10)[0]\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "# sort reccomendations by sentiment\n",
    "sorted_reccomendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "# Print the recommendations with place names without rand_ind\n",
    "print(sorted_reccomendations[['name','types_x', 'rating']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T12:26:43.912890900Z",
     "start_time": "2024-06-17T12:26:03.363619500Z"
    }
   },
   "id": "ec9cc8cc4f034276",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    # Encode the 'types' column\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Prepare the input features\n",
    "    X_review = padded_sequences\n",
    "    X_types = data['types_encoded'].values\n",
    "\n",
    "    # Get the index of the specified place_id\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X_review[place_idx]\n",
    "    place_types = X_types[place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X_review, X_types], batch_size=128, verbose=0)\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X_review, X_types.reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n-1:][::-1]\n",
    "    similar_indices = similar_indices[similar_indices != place_idx][:top_n]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('39_test_modelV3.keras')\n",
    "\n",
    "# Load the datasets\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "# Get a random place ID\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place: {rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "# Get recommendations\n",
    "recommendations, predicted_sentiments = get_recommendations('ChIJQ5jInls_0i0Ra53iWVquuq8', df_review, model, top_n=10)\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "\n",
    "# Sort recommendations by sentiment\n",
    "sorted_recommendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "\n",
    "# Print the recommendations with place names without rand_id\n",
    "print(sorted_recommendations[['name', 'types_x', 'rating']])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3393d67ac8206c23"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import faiss\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    # Encode the 'types' column\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Prepare the input features\n",
    "    X_review = padded_sequences\n",
    "    X_types = data['types_encoded'].values\n",
    "\n",
    "    # Get the index of the specified place_id\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X_review[place_idx]\n",
    "    place_types = X_types[place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X_review, X_types], batch_size=128, verbose=0)\n",
    "\n",
    "    # Combine review and types vectors\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X_review, X_types.reshape(-1, 1)])\n",
    "\n",
    "    # Using Faiss for approximate nearest neighbors\n",
    "    d = all_vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(all_vectors.astype(np.float32))\n",
    "    D, I = index.search(np.array([place_vector.astype(np.float32)]), top_n + 1)\n",
    "\n",
    "    # Get top N similar places (excluding the place itself)\n",
    "    similar_indices = I[0][I[0] != place_idx][:top_n]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('39_test_modelV3.keras')\n",
    "\n",
    "# Load the datasets\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "# Get a random place ID\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place: {rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "# Get recommendations\n",
    "recommendations, predicted_sentiments = get_recommendations('ChIJQ5jInls_0i0Ra53iWVquuq8', df_review, model, top_n=10)\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "\n",
    "# Sort recommendations by sentiment\n",
    "sorted_recommendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "\n",
    "# Print the recommendations with place names without rand_id\n",
    "print(sorted_recommendations[['name', 'types_x', 'rating']])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf354cb3cba2942c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 00:47:52.505285: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-16 00:47:52.614954: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-16 00:47:52.615008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-16 00:47:52.616515: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-16 00:47:52.625781: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 00:47:54.334906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tokenizer\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequence\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pad_sequences\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfaiss\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Function to get recommendations based on a place ID\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_recommendations\u001B[39m(place_id, data, model, top_n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;66;03m# Encode the 'types' column\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import faiss\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    # Encode the 'types' column\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Prepare the input features\n",
    "    X_review = padded_sequences\n",
    "    X_types = data['types_encoded'].values.reshape(-1, 1)\n",
    "\n",
    "    # Combine review and typ es vectors\n",
    "    combined_vectors = np.hstack([X_review, X_types])\n",
    "\n",
    "    # Dimensionality reduction using PCA\n",
    "    pca = PCA(n_components=50)  # Adjust the number of components as needed\n",
    "    reduced_vectors = pca.fit_transform(combined_vectors)\n",
    "\n",
    "    # Get the index of the specified place_id\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_vector = reduced_vectors[place_idx]\n",
    "\n",
    "    # Using Faiss for approximate nearest neighbors\n",
    "    d = reduced_vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(reduced_vectors.astype(np.float32))\n",
    "    D, I = index.search(np.array([place_vector.astype(np.float32)]), top_n + 1)\n",
    "\n",
    "    # Get top N similar places (excluding the place itself)\n",
    "    similar_indices = I[0][I[0] != place_idx][:top_n]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X_review, X_types.squeeze()], batch_size=128, verbose=0)\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('39_test_modelV3.keras')\n",
    "\n",
    "# Load the datasets\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "# Get a random place ID\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place: {rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "# Get recommendations\n",
    "recommendations, predicted_sentiments = get_recommendations('ChIJQ5jInls_0i0Ra53iWVquuq8', df_review, model, top_n=10)\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "\n",
    "# Sort recommendations by sentiment\n",
    "sorted_recommendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "\n",
    "# Print the recommendations with place names without rand_id\n",
    "print(sorted_recommendations[['name', 'types_x', 'rating']])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T17:47:57.010043800Z",
     "start_time": "2024-06-15T17:47:51.785870Z"
    }
   },
   "id": "8e0d46303def5fe1",
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T17:47:34.870730300Z",
     "start_time": "2024-06-15T17:47:34.850158500Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1;31m2024-06-16 00:47:34.827301102 [E:onnxruntime:Default, provider_bridge_ort.cc:1744 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "\u001B[m\n",
      "\u001B[0;93m2024-06-16 00:47:34.827376805 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:870 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001B[m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01monnxruntime\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mort\u001B[39;00m\n\u001B[1;32m      3\u001B[0m session \u001B[38;5;241m=\u001B[39m ort\u001B[38;5;241m.\u001B[39mInferenceSession(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest.onnx\u001B[39m\u001B[38;5;124m'\u001B[39m, providers\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCUDAExecutionProvider\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m----> 4\u001B[0m results_ort \u001B[38;5;241m=\u001B[39m session\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;28;01mNone\u001B[39;00m, {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtypes\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mX\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtypes\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreview\u001B[39m\u001B[38;5;124m\"\u001B[39m: X[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreview\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)})\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(X[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtypes\u001B[39m\u001B[38;5;124m'\u001B[39m]))\n\u001B[1;32m      6\u001B[0m X[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtypes\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X' is not defined"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession('test.onnx', providers=['CUDAExecutionProvider'])\n",
    "results_ort = session.run(None, {\"types\": X['types'].reshape(-1, 1), \"review\": X['review'].astype(np.float32)})\n",
    "print(type(X['types']))\n",
    "X['types']"
   ],
   "id": "12ea25b9e40ff802"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7528b43b6289943a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
