{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# from keras_tuner import RandomSearch, HyperModel\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'combined-dataset/final_reviews_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode the 'types' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "\n",
    "# Tokenize the 'review' column\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "\n",
    "# Pad the sequences\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create the feature set\n",
    "X = {\n",
    "    'review': padded_sequences,\n",
    "    'types': data['types_encoded'].values,\n",
    "}\n",
    "\n",
    "# Normalize the sentiment scores\n",
    "y = data['sentiment'].values\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T18:50:37.271635400Z",
     "start_time": "2024-06-14T18:50:26.572468200Z"
    }
   },
   "id": "4fce0614da3d08b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anton\\PycharmProjects\\BangkitStuff\\capstone-ngelana\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Define input layers\n",
    "review_input = Input(shape=(max_sequence_length,), name='review')\n",
    "types_input = Input(shape=(1,), name='types')\n",
    "\n",
    "# Define embedding and LSTM layers for review input\n",
    "review_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(review_input)\n",
    "review_lstm = LSTM(128)(review_embedding)\n",
    "\n",
    "# Define embedding layer for types input\n",
    "types_embedding = Embedding(input_dim=data['types_encoded'].nunique(), output_dim=10)(types_input)\n",
    "types_flat = Flatten()(types_embedding)\n",
    "\n",
    "# Concatenate the review and types embeddings\n",
    "concatenated = Concatenate()([review_lstm, types_flat])\n",
    "\n",
    "# Sequential part of the model\n",
    "sequential_model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(concatenated.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Full model combining the inputs and sequential model\n",
    "output = sequential_model(concatenated)\n",
    "full_model = Model(inputs=[review_input, types_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "full_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "full_model.fit([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "full_model.save('39_test_modelV2.keras')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-14T18:50:37.274612800Z"
    }
   },
   "id": "16764ec0f1acbb1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anton\\PycharmProjects\\BangkitStuff\\capstone-ngelana\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "WARNING:tensorflow:From C:\\Users\\anton\\PycharmProjects\\BangkitStuff\\capstone-ngelana\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\anton\\PycharmProjects\\BangkitStuff\\capstone-ngelana\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From C:\\Users\\anton\\PycharmProjects\\BangkitStuff\\capstone-ngelana\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "205/819 [======>.......................] - ETA: 4:45 - loss: 0.7424 - mae: 0.5008"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T18:50:20.722074Z",
     "start_time": "2024-06-14T18:50:20.721076900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class SentimentHyperModel(HyperModel):\n",
    "#     def build(self, hp):\n",
    "#         review_input = Input(shape=(max_sequence_length,), name='review')\n",
    "#         types_input = Input(shape=(1,), name='types')\n",
    "# \n",
    "#         # Define embedding and LSTM layers for review input\n",
    "#         embedding_output_dim = hp.Int('embedding_output_dim', min_value=64, max_value=256, step=32)\n",
    "#         lstm_units = hp.Int('lstm_units', min_value=64, max_value=256, step=32)\n",
    "#         review_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_output_dim)(review_input)\n",
    "#         review_lstm = LSTM(units=lstm_units)(review_embedding)\n",
    "# \n",
    "#         # Define embedding layer for types input\n",
    "#         types_embedding = Embedding(input_dim=data['types_encoded'].nunique(), output_dim=10)(types_input)\n",
    "#         types_flat = Flatten()(types_embedding)\n",
    "# \n",
    "#         # Concatenate the review and types embeddings\n",
    "#         concatenated = Concatenate()([review_lstm, types_flat])\n",
    "# \n",
    "#         # Add dense layers for final prediction\n",
    "#         dense_units_1 = hp.Int('dense_units_1', min_value=64, max_value=256, step=32)\n",
    "#         dense_units_2 = hp.Int('dense_units_2', min_value=32, max_value=128, step=16)\n",
    "#         dense_1 = Dense(units=dense_units_1, activation='relu')(concatenated)\n",
    "#         dense_2 = Dense(units=dense_units_2, activation='relu')(dense_1)\n",
    "#         output = Dense(1, activation='linear')(dense_2)\n",
    "# \n",
    "#         # Choose an optimizer\n",
    "#         optimizer_choice = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
    "# \n",
    "#         if optimizer_choice == 'adam':\n",
    "#             optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "#         elif optimizer_choice == 'sgd':\n",
    "#             optimizer = tf.keras.optimizers.SGD(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "#         elif optimizer_choice == 'rmsprop':\n",
    "#             optimizer = tf.keras.optimizers.RMSprop(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "# \n",
    "#         # Create the model\n",
    "#         model = Model(inputs=[review_input, types_input], outputs=output)\n",
    "#         model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "# \n",
    "#         return model"
   ],
   "id": "7a8333f8e323cb4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-14T18:50:20.724073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tuner = RandomSearch(\n",
    "#     hypermodel=SentimentHyperModel(),\n",
    "#     objective='val_loss',\n",
    "#     max_trials=10,  # Number of different hyperparameter sets to try\n",
    "#     executions_per_trial=2,  # Number of models to train with the same hyperparameters\n",
    "#     directory='model-testing',\n",
    "#     project_name='sentiment_tuning'\n",
    "# )\n",
    "# \n",
    "# # %%\n",
    "# # Search for the best hyperparameters\n",
    "# tuner.search([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# \n",
    "# # Get the optimal hyperparameters\n",
    "# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "# \n",
    "# # Build the best model\n",
    "# best_model = tuner.hypermodel.build(best_hps)"
   ],
   "id": "633172108e147ab3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T18:50:20.738613900Z",
     "start_time": "2024-06-14T18:50:20.726075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# best_model.fit([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# \n",
    "# # %%\n",
    "# # Save the best model\n",
    "# best_model.save('best_sentiment_model.keras')"
   ],
   "id": "2d8966f7cb395973",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-14T18:50:20.727070200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('39_test_model.keras')"
   ],
   "id": "8bf1e691a2787bfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X['review'][place_idx]\n",
    "    place_types = X['types'][place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X['review'], X['types']])\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X['review'], X['types'].reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Example \n",
    "place_id = 'ChIJIaGQ-Eg60i0RnT9pzyD_gvM'  # Replace with an actual place ID from your dataset\n",
    "recommendations = get_recommendations(place_id, data, model, top_n=10)\n",
    "print(recommendations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-14T18:50:20.729602900Z"
    }
   },
   "id": "ade66f54ef88bcd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "model.save('39_test_model.keras')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-14T18:50:20.731631Z"
    }
   },
   "id": "6c28122d45aa642c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "# model = tf.keras.models.load_model('39_test_model.keras')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-14T18:50:20.733617Z"
    }
   },
   "id": "8f8b901fff697f59",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place :{rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "recommendations = get_recommendations(rand_id, df_review, model, top_n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-14T18:50:20.735615600Z"
    }
   },
   "id": "73f3d0d247699c85",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "# sort reccomendations by sentiment\n",
    "sorted_reccomendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "# Print the recommendations with place names with out rand_ind\n",
    "print(sorted_reccomendations[['name','types_x', 'rating']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-14T18:50:20.737614600Z"
    }
   },
   "id": "c6dc751da34a7a4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Test cell, Run this cell to get recommendations for a random place in the dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Flatten\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Create the feature set\n",
    "    X = {\n",
    "        'review': padded_sequences,\n",
    "        'types': data['types_encoded'].values,\n",
    "    }\n",
    "    \n",
    "    # Normalize the sentiment scores\n",
    "    y = data['sentiment'].values\n",
    "\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X['review'][place_idx]\n",
    "    place_types = X['types'][place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X['review'], X['types']])\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X['review'], X['types'].reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# model = tf.keras.models.load_model('best_sentiment_model.keras')\n",
    "model = tf.keras.models.load_model('39_test_model.keras')\n",
    "\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place :{rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "recommendations = get_recommendations(rand_id, df_review, model, top_n=10)\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "# sort reccomendations by sentiment\n",
    "sorted_reccomendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "# Print the recommendations with place names with out rand_ind\n",
    "print(sorted_reccomendations[['name','types_x', 'rating']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-14T18:50:20.738613900Z"
    }
   },
   "id": "774efa3b0ffded2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T18:50:20.741143200Z",
     "start_time": "2024-06-14T18:50:20.741143200Z"
    }
   },
   "id": "a3e19ef7827c63e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
