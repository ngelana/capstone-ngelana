{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# from keras_tuner import RandomSearch, HyperModel\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'combined-dataset/final_reviews_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode the 'types' column\n",
    "label_encoder = LabelEncoder()\n",
    "data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "\n",
    "# Tokenize the 'review' column\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "\n",
    "# Pad the sequences\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create the feature set\n",
    "X = {\n",
    "    'review': padded_sequences,\n",
    "    'types': data['types_encoded'].values,\n",
    "}\n",
    "\n",
    "# Normalize the sentiment scores\n",
    "y = data['sentiment'].values\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T11:24:55.689772400Z",
     "start_time": "2024-06-15T11:24:51.475686700Z"
    }
   },
   "id": "4fce0614da3d08b4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "819/819 [==============================] - 521s 633ms/step - loss: 0.3355 - mae: 0.3480 - val_loss: 0.0974 - val_mae: 0.2240\n",
      "Epoch 2/10\n",
      "819/819 [==============================] - 473s 578ms/step - loss: 0.0500 - mae: 0.1582 - val_loss: 0.0479 - val_mae: 0.1485\n",
      "Epoch 3/10\n",
      "819/819 [==============================] - 444s 542ms/step - loss: 0.0252 - mae: 0.1104 - val_loss: 0.0495 - val_mae: 0.1689\n",
      "Epoch 4/10\n",
      "819/819 [==============================] - 459s 561ms/step - loss: 0.0149 - mae: 0.0839 - val_loss: 0.0318 - val_mae: 0.1177\n",
      "Epoch 5/10\n",
      "819/819 [==============================] - 483s 590ms/step - loss: 0.0107 - mae: 0.0718 - val_loss: 0.0309 - val_mae: 0.1143\n",
      "Epoch 6/10\n",
      "819/819 [==============================] - 509s 622ms/step - loss: 0.0084 - mae: 0.0638 - val_loss: 0.0352 - val_mae: 0.1367\n",
      "Epoch 7/10\n",
      "819/819 [==============================] - 564s 689ms/step - loss: 0.0071 - mae: 0.0592 - val_loss: 0.0263 - val_mae: 0.1058\n",
      "Epoch 8/10\n",
      "819/819 [==============================] - 516s 630ms/step - loss: 0.0060 - mae: 0.0545 - val_loss: 0.0243 - val_mae: 0.1024\n",
      "Epoch 9/10\n",
      "819/819 [==============================] - 487s 595ms/step - loss: 0.0050 - mae: 0.0499 - val_loss: 0.0232 - val_mae: 0.1008\n",
      "Epoch 10/10\n",
      "819/819 [==============================] - 465s 568ms/step - loss: 0.0044 - mae: 0.0471 - val_loss: 0.0221 - val_mae: 0.0998\n"
     ]
    }
   ],
   "source": [
    "# Define input layers\n",
    "review_input = Input(shape=(max_sequence_length,), name='review')\n",
    "types_input = Input(shape=(1,), name='types')\n",
    "\n",
    "# Define embedding and LSTM layers for review input\n",
    "review_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)(review_input)\n",
    "review_lstm = LSTM(128)(review_embedding)\n",
    "\n",
    "# Define embedding layer for types input\n",
    "types_embedding = Embedding(input_dim=data['types_encoded'].nunique(), output_dim=10)(types_input)\n",
    "types_flat = Flatten()(types_embedding)\n",
    "\n",
    "# Concatenate the review and types embeddings\n",
    "concatenated = Concatenate()([review_lstm, types_flat])\n",
    "\n",
    "# Sequential part of the model\n",
    "sequential_model = Sequential([\n",
    "    Input(shape=(concatenated.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Full model combining the inputs and sequential model\n",
    "output = sequential_model(concatenated)\n",
    "full_model = Model(inputs=[review_input, types_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "full_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "full_model.fit([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "full_model.save('39_test_modelV3.keras')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T12:46:58.670151Z",
     "start_time": "2024-06-15T11:24:55.698771300Z"
    }
   },
   "id": "504cbee8d23dd9b6",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# class SentimentHyperModel(HyperModel):\n",
    "#     def build(self, hp):\n",
    "#         review_input = Input(shape=(max_sequence_length,), name='review')\n",
    "#         types_input = Input(shape=(1,), name='types')\n",
    "# \n",
    "#         # Define embedding and LSTM layers for review input\n",
    "#         embedding_output_dim = hp.Int('embedding_output_dim', min_value=64, max_value=256, step=32)\n",
    "#         lstm_units = hp.Int('lstm_units', min_value=64, max_value=256, step=32)\n",
    "#         review_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_output_dim)(review_input)\n",
    "#         review_lstm = LSTM(units=lstm_units)(review_embedding)\n",
    "# \n",
    "#         # Define embedding layer for types input\n",
    "#         types_embedding = Embedding(input_dim=data['types_encoded'].nunique(), output_dim=10)(types_input)\n",
    "#         types_flat = Flatten()(types_embedding)\n",
    "# \n",
    "#         # Concatenate the review and types embeddings\n",
    "#         concatenated = Concatenate()([review_lstm, types_flat])\n",
    "# \n",
    "#         # Add dense layers for final prediction\n",
    "#         dense_units_1 = hp.Int('dense_units_1', min_value=64, max_value=256, step=32)\n",
    "#         dense_units_2 = hp.Int('dense_units_2', min_value=32, max_value=128, step=16)\n",
    "#         dense_1 = Dense(units=dense_units_1, activation='relu')(concatenated)\n",
    "#         dense_2 = Dense(units=dense_units_2, activation='relu')(dense_1)\n",
    "#         output = Dense(1, activation='linear')(dense_2)\n",
    "# \n",
    "#         # Choose an optimizer\n",
    "#         optimizer_choice = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
    "# \n",
    "#         if optimizer_choice == 'adam':\n",
    "#             optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "#         elif optimizer_choice == 'sgd':\n",
    "#             optimizer = tf.keras.optimizers.SGD(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "#         elif optimizer_choice == 'rmsprop':\n",
    "#             optimizer = tf.keras.optimizers.RMSprop(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log'))\n",
    "# \n",
    "#         # Create the model\n",
    "#         model = Model(inputs=[review_input, types_input], outputs=output)\n",
    "#         model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "# \n",
    "#         return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T10:24:57.480027500Z",
     "start_time": "2024-06-15T10:24:57.478927500Z"
    }
   },
   "id": "9521ee212207efea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tuner = RandomSearch(\n",
    "#     hypermodel=SentimentHyperModel(),\n",
    "#     objective='val_loss',\n",
    "#     max_trials=10,  # Number of different hyperparameter sets to try\n",
    "#     executions_per_trial=2,  # Number of models to train with the same hyperparameters\n",
    "#     directory='model-testing',\n",
    "#     project_name='sentiment_tuning'\n",
    "# )\n",
    "# \n",
    "# # %%\n",
    "# # Search for the best hyperparameters\n",
    "# tuner.search([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# \n",
    "# # Get the optimal hyperparameters\n",
    "# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "# \n",
    "# # Build the best model\n",
    "# best_model = tuner.hypermodel.build(best_hps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-15T10:24:57.482177900Z"
    }
   },
   "id": "ae6f44a7ccf071a3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# best_model.fit([X['review'], X['types']], y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# \n",
    "# # %%\n",
    "# # Save the best model\n",
    "# best_model.save('best_sentiment_model.keras')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T10:24:57.487875500Z",
     "start_time": "2024-06-15T10:24:57.485482900Z"
    }
   },
   "id": "dd7c657b61eaf372"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('39_test_modelV2.keras')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T10:24:57.488961700Z",
     "start_time": "2024-06-15T10:24:57.487875500Z"
    }
   },
   "id": "4e978270b0a0017a"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X['review'][place_idx]\n",
    "    place_types = X['types'][place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X['review'], X['types']])\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X['review'], X['types'].reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Example \n",
    "place_id = 'ChIJIaGQ-Eg60i0RnT9pzyD_gvM'  # Replace with an actual place ID from your dataset\n",
    "recommendations = get_recommendations(place_id, data, model, top_n=10)\n",
    "print(recommendations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-15T10:24:57.490239100Z"
    }
   },
   "id": "ade66f54ef88bcd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "model.save('39_test_model.keras')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-15T10:24:57.493468Z"
    }
   },
   "id": "6c28122d45aa642c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "# model = tf.keras.models.load_model('39_test_model.keras')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T10:24:57.495814700Z",
     "start_time": "2024-06-15T10:24:57.495814700Z"
    }
   },
   "id": "8f8b901fff697f59",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place :{rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "recommendations = get_recommendations(rand_id, df_review, model, top_n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-15T10:24:57.497627600Z"
    }
   },
   "id": "73f3d0d247699c85",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "# sort reccomendations by sentiment\n",
    "sorted_reccomendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "# Print the recommendations with place names with out rand_ind\n",
    "print(sorted_reccomendations[['name','types_x', 'rating']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T10:24:57.531132500Z",
     "start_time": "2024-06-15T10:24:57.500139100Z"
    }
   },
   "id": "c6dc751da34a7a4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Test cell, Run this cell to get recommendations for a random place in the dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Flatten\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Create the feature set\n",
    "    X = {\n",
    "        'review': padded_sequences,\n",
    "        'types': data['types_encoded'].values,\n",
    "    }\n",
    "    \n",
    "    # Normalize the sentiment scores\n",
    "    y = data['sentiment'].values\n",
    "\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X['review'][place_idx]\n",
    "    place_types = X['types'][place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X['review'], X['types']])\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X['review'], X['types'].reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# model = tf.keras.models.load_model('best_sentiment_model.keras')\n",
    "model = tf.keras.models.load_model('39_test_modelV3.keras')\n",
    "\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place :{rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "recommendations = get_recommendations('ChIJQ5jInls_0i0Ra53iWVquuq8', df_review, model, top_n=10)[0]\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "# sort reccomendations by sentiment\n",
    "sorted_reccomendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "# Print the recommendations with place names with out rand_ind\n",
    "print(sorted_reccomendations[['name','types_x', 'rating']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T13:13:45.969324800Z",
     "start_time": "2024-06-15T13:11:53.061266Z"
    }
   },
   "id": "774efa3b0ffded2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n",
      "Random place :ChIJF70-LKBG0i0RUO3m3-Jy9BM The Anvaya Beach Resort Bali\n",
      "1024/1024 [==============================] - 106s 103ms/step\n",
      "                                  name  \\\n",
      "2              Teba Junjungan Cottages   \n",
      "5  Griya Santrian a Beach Resort & Spa   \n",
      "9              Goa Rang Reng Waterfall   \n",
      "6                    Matahari Bungalow   \n",
      "4                         Umah ketipat   \n",
      "8                       Hot Stone Club   \n",
      "1                    Wyn's Penida Cafe   \n",
      "0   Karsawabali Khos dan Innoka Coffee   \n",
      "7                Wr. Nasi Lukluk (WNL)   \n",
      "3            Warung Banyuwangi Bu Doni   \n",
      "\n",
      "                                             types_x  rating  \n",
      "2                         bed_and_breakfast, lodging     4.5  \n",
      "5  hotel, resort_hotel, spa, lodging, restaurant,...     4.5  \n",
      "9                                 tourist_attraction     4.7  \n",
      "6                                            lodging     4.2  \n",
      "4                                   restaurant, food     4.5  \n",
      "8  fitness_center, hotel, spa, gym, lodging, spor...     4.6  \n",
      "1                                   restaurant, food     4.7  \n",
      "0  coffee_shop, private_guest_room, cafe, lodging...     4.8  \n",
      "7  breakfast_restaurant, indonesian_restaurant, r...     4.4  \n",
      "3            indonesian_restaurant, restaurant, food     4.6  \n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    # Encode the 'types' column\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Prepare the input features\n",
    "    X_review = padded_sequences\n",
    "    X_types = data['types_encoded'].values\n",
    "\n",
    "    # Get the index of the specified place_id\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X_review[place_idx]\n",
    "    place_types = X_types[place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X_review, X_types], batch_size=128, verbose=0)\n",
    "\n",
    "    # Calculate similarity\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X_review, X_types.reshape(-1, 1)])\n",
    "    similarities = cosine_similarity([place_vector], all_vectors)[0]\n",
    "\n",
    "    # Get top N similar places\n",
    "    similar_indices = np.argsort(similarities)[-top_n-1:][::-1]\n",
    "    similar_indices = similar_indices[similar_indices != place_idx][:top_n]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('39_test_modelV3.keras')\n",
    "\n",
    "# Load the datasets\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "# Get a random place ID\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place: {rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "# Get recommendations\n",
    "recommendations, predicted_sentiments = get_recommendations('ChIJQ5jInls_0i0Ra53iWVquuq8', df_review, model, top_n=10)\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "\n",
    "# Sort recommendations by sentiment\n",
    "sorted_recommendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "\n",
    "# Print the recommendations with place names without rand_id\n",
    "print(sorted_recommendations[['name', 'types_x', 'rating']])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T13:16:31.122021600Z",
     "start_time": "2024-06-15T13:13:45.966275900Z"
    }
   },
   "id": "a3e19ef7827c63e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random place: ChIJ04MsoFw60i0Re1uww-1VfY0 Warung DKI\n",
      "                                  name  \\\n",
      "1              Teba Junjungan Cottages   \n",
      "4  Griya Santrian a Beach Resort & Spa   \n",
      "8              Goa Rang Reng Waterfall   \n",
      "5                    Matahari Bungalow   \n",
      "3                         Umah ketipat   \n",
      "7                       Hot Stone Club   \n",
      "0                    Wyn's Penida Cafe   \n",
      "6                Wr. Nasi Lukluk (WNL)   \n",
      "9                  The Upper Deck Ubud   \n",
      "2            Warung Banyuwangi Bu Doni   \n",
      "\n",
      "                                             types_x  rating  \n",
      "1                         bed_and_breakfast, lodging     4.5  \n",
      "4  hotel, resort_hotel, spa, lodging, restaurant,...     4.5  \n",
      "8                                 tourist_attraction     4.7  \n",
      "5                                            lodging     4.2  \n",
      "3                                   restaurant, food     4.5  \n",
      "7  fitness_center, hotel, spa, gym, lodging, spor...     4.6  \n",
      "0                                   restaurant, food     4.7  \n",
      "6  breakfast_restaurant, indonesian_restaurant, r...     4.4  \n",
      "9                                   restaurant, food     4.8  \n",
      "2            indonesian_restaurant, restaurant, food     4.6  \n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random place: ChIJ3bZRNeZA0i0RAYfkeurNqDo Wiracana HandFan Sesetan\n",
      "                                   name  \\\n",
      "1               Teba Junjungan Cottages   \n",
      "3   Griya Santrian a Beach Resort & Spa   \n",
      "8                     Salad Lab Ungasan   \n",
      "2                          Umah ketipat   \n",
      "6  Abian Harmony Hotel Restaurant & Spa   \n",
      "9                        Hot Stone Club   \n",
      "7             Sea View Villa Bloom Bali   \n",
      "0                     Wyn's Penida Cafe   \n",
      "5                     Cici Claypot Bali   \n",
      "4                 Wr. Nasi Lukluk (WNL)   \n",
      "\n",
      "                                             types_x  rating  \n",
      "1                         bed_and_breakfast, lodging     4.5  \n",
      "3  hotel, resort_hotel, spa, lodging, restaurant,...     4.5  \n",
      "8                                   restaurant, food     4.4  \n",
      "2                                   restaurant, food     4.5  \n",
      "6                                     hotel, lodging     3.9  \n",
      "9  fitness_center, hotel, spa, gym, lodging, spor...     4.6  \n",
      "7                  bed_and_breakfast, hotel, lodging     4.9  \n",
      "0                                   restaurant, food     4.7  \n",
      "5                                   restaurant, food     4.8  \n",
      "4  breakfast_restaurant, indonesian_restaurant, r...     4.4  \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import faiss\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    # Encode the 'types' column\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Prepare the input features\n",
    "    X_review = padded_sequences\n",
    "    X_types = data['types_encoded'].values\n",
    "\n",
    "    # Get the index of the specified place_id\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_review = X_review[place_idx]\n",
    "    place_types = X_types[place_idx]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X_review, X_types], batch_size=128, verbose=0)\n",
    "\n",
    "    # Combine review and types vectors\n",
    "    place_vector = np.concatenate([place_review, [place_types]])\n",
    "    all_vectors = np.hstack([X_review, X_types.reshape(-1, 1)])\n",
    "\n",
    "    # Using Faiss for approximate nearest neighbors\n",
    "    d = all_vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(all_vectors.astype(np.float32))\n",
    "    D, I = index.search(np.array([place_vector.astype(np.float32)]), top_n + 1)\n",
    "\n",
    "    # Get top N similar places (excluding the place itself)\n",
    "    similar_indices = I[0][I[0] != place_idx][:top_n]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('39_test_modelV3.keras')\n",
    "\n",
    "# Load the datasets\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "# Get a random place ID\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place: {rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "# Get recommendations\n",
    "recommendations, predicted_sentiments = get_recommendations('ChIJQ5jInls_0i0Ra53iWVquuq8', df_review, model, top_n=10)\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "\n",
    "# Sort recommendations by sentiment\n",
    "sorted_recommendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "\n",
    "# Print the recommendations with place names without rand_id\n",
    "print(sorted_recommendations[['name', 'types_x', 'rating']])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T13:18:23.027429400Z",
     "start_time": "2024-06-15T13:16:31.128972500Z"
    }
   },
   "id": "2c27c559c0b90e3e",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random place: ChIJGwTW4RlH0i0RSU-EOx8spQ0 Amadea Resort & Villas\n",
      "                               name  \\\n",
      "2                 Salad Lab Ungasan   \n",
      "5     Dwa Chandra Villas & Retreats   \n",
      "7  Marriottâ€™s Bali Nusa Dua Terrace   \n",
      "0                   Ubud Batan Nyuh   \n",
      "8                    Ritatkala Cafe   \n",
      "4                    Ubud Poke Bowl   \n",
      "3                    INFINITY8 BALI   \n",
      "1               Alassari Plantation   \n",
      "6                      Villa Mathis   \n",
      "9               Taman Bintang Villa   \n",
      "\n",
      "                                             types_x  rating  \n",
      "2                                   restaurant, food     4.4  \n",
      "5    bed_and_breakfast, event_venue, lodging, health     5.0  \n",
      "7         hotel, wedding_venue, lodging, event_venue     4.7  \n",
      "0                                     hotel, lodging     4.3  \n",
      "8  cafe, coffee_shop, indonesian_restaurant, stor...     4.8  \n",
      "4                indian_restaurant, restaurant, food     4.7  \n",
      "3  hotel, bed_and_breakfast, coffee_shop, swimmin...     4.4  \n",
      "1                                     hotel, lodging     4.4  \n",
      "6                                     hotel, lodging     4.2  \n",
      "9                                     hotel, lodging     4.4  \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import faiss\n",
    "\n",
    "# Function to get recommendations based on a place ID\n",
    "def get_recommendations(place_id, data, model, top_n=10):\n",
    "    # Encode the 'types' column\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['types_encoded'] = label_encoder.fit_transform(data['types'])\n",
    "    \n",
    "    # Tokenize the 'review' column\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data['review'])\n",
    "    sequences = tokenizer.texts_to_sequences(data['review'])\n",
    "    \n",
    "    # Pad the sequences\n",
    "    max_sequence_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Prepare the input features\n",
    "    X_review = padded_sequences\n",
    "    X_types = data['types_encoded'].values.reshape(-1, 1)\n",
    "\n",
    "    # Combine review and types vectors\n",
    "    combined_vectors = np.hstack([X_review, X_types])\n",
    "\n",
    "    # Dimensionality reduction using PCA\n",
    "    pca = PCA(n_components=50)  # Adjust the number of components as needed\n",
    "    reduced_vectors = pca.fit_transform(combined_vectors)\n",
    "\n",
    "    # Get the index of the specified place_id\n",
    "    place_idx = data[data['id'] == place_id].index[0]\n",
    "    place_vector = reduced_vectors[place_idx]\n",
    "\n",
    "    # Using Faiss for approximate nearest neighbors\n",
    "    d = reduced_vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(reduced_vectors.astype(np.float32))\n",
    "    D, I = index.search(np.array([place_vector.astype(np.float32)]), top_n + 1)\n",
    "\n",
    "    # Get top N similar places (excluding the place itself)\n",
    "    similar_indices = I[0][I[0] != place_idx][:top_n]\n",
    "    similar_places = data.iloc[similar_indices]\n",
    "\n",
    "    # Predict the sentiment for all places\n",
    "    predicted_sentiments = model.predict([X_review, X_types.squeeze()], batch_size=128, verbose=0)\n",
    "\n",
    "    return similar_places, predicted_sentiments[similar_indices]\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('39_test_modelV3.keras')\n",
    "\n",
    "# Load the datasets\n",
    "df_review = pd.read_csv('combined-dataset/final_reviews_data.csv')\n",
    "df_place = pd.read_csv('combined-dataset/combined_datasetV2.csv')\n",
    "\n",
    "# Get a random place ID\n",
    "random_place = df_review.sample(1)\n",
    "rand_id = random_place['id'].values[0]\n",
    "print(f'Random place: {rand_id}', df_place[df_place['id'] == rand_id]['name'].values[0])\n",
    "\n",
    "# Get recommendations\n",
    "recommendations, predicted_sentiments = get_recommendations('ChIJQ5jInls_0i0Ra53iWVquuq8', df_review, model, top_n=10)\n",
    "\n",
    "# Merge the recommendations with place names based on 'id'\n",
    "merged_recommendations = recommendations.merge(df_place, on='id')\n",
    "\n",
    "# Sort recommendations by sentiment\n",
    "sorted_recommendations = merged_recommendations.sort_values(by='sentiment', ascending=False)\n",
    "\n",
    "# Print the recommendations with place names without rand_id\n",
    "print(sorted_recommendations[['name', 'types_x', 'rating']])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-15T13:19:59.520765900Z",
     "start_time": "2024-06-15T13:18:23.032310900Z"
    }
   },
   "id": "2d649be0886573c4",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "35772f48f6ee90f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
